{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT默认配置\n",
    "\n",
    "```python\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        qkv_bias=True,\n",
    "        encoder_stride=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "# Initializing a ViT vit-base-patch16-224 style configuration\n",
    "configuration = ViTConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
    "model = ViTModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用预训练的ViT模型处理图像数据，并提取图像的最后一层隐藏状态。这种隐藏状态可以用于各种任务，如图像分类、特征提取等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 173k/173k [00:00<00:00, 1.27MB/s]\n",
      "Generating test split: 1 examples [00:00, 30.01 examples/s]\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Error while downloading from https://cdn-lfs.hf.co/google/vit-base-patch16-224-in21k/fd4e1169c7aa6c2dbfa8a6448be13b35abc0ee256190857c90009d12c094619b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1740626380&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDYyNjM4MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9nb29nbGUvdml0LWJhc2UtcGF0Y2gxNi0yMjQtaW4yMWsvZmQ0ZTExNjljN2FhNmMyZGJmYThhNjQ0OGJlMTNiMzVhYmMwZWUyNTYxOTA4NTdjOTAwMDlkMTJjMDk0NjE5Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=UabnbSYRBjfUOtyXzEtzyp921MmH3D5Jwdf8u7pPHXa1H7D5d9ezMN5MkNcvsoCLNg1TqqgGWyEjO16H6Alsix3l3CMdH%7EcjdTM4KBXxaMuXvlOgw4LkEz%7E3uUqQrsO8ED34Wymm6j338ji5pxlsvL7ZISucu1%7ExpoAcRkVKqKuETrHs6hqAu5KMJgA6aDDG9Y%7EfgL9OMmAsrPGKCQnVzIqV1JN7rZEmuyQJXhXr0NlHggOydxDTcNIFoUE4qL3AVyeu-QH-iElvOvLydmo0lnEQ3hriwXxPECNoIzg60QdBDsOe%7EMcqrWOzulSeFlDnHoKru6Yh3SwIkkZbPcLaBA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 197, 768]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad(): # 禁用梯度计算\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载一个预训练的视觉Transformer（ViT）模型，并对一张图像进行掩码图像建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTForMaskedImageModeling were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['decoder.0.bias', 'decoder.0.weight', 'embeddings.mask_token']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本的图片大小:  (640, 480)\n",
      "patch大小:  16\n",
      "patch数量:  196\n",
      "1表示批量大小，这里只处理一张图像。3表示图像的通道数，通常是RGB图像的三个通道。224*224表示重建像素值的形状。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForMaskedImageModeling\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# 加载预训练的图像处理器\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# 加载预训练的ViT模型\n",
    "model = ViTForMaskedImageModeling.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# 根据图像大小和patch大小计算图像被分割成的patch数量\n",
    "print(\"原本的图片大小: \", image.size)\n",
    "print(\"patch大小: \", model.config.patch_size)\n",
    "\n",
    "num_patches = (model.config.image_size // model.config.patch_size) ** 2\n",
    "print(\"patch数量: \", num_patches)\n",
    "\n",
    "# 使用图像处理器将图像转换为模型所需的像素值张量\n",
    "pixel_values = image_processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "# 生成一个随机的布尔掩码，用于指定哪些patch将被掩码\n",
    "bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()\n",
    "\n",
    "# 输出重建像素值的形状\n",
    "outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction\n",
    "list(reconstructed_pixel_values.shape)\n",
    "\n",
    "print(\"1表示批量大小，这里只处理一张图像。3表示图像的通道数，通常是RGB图像的三个通道。224*224表示重建像素值的形状。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载一个预训练的视觉Transformer（ViT）模型，并对一张猫的图像进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n",
      "Egyptian cat\n",
      "Egyptian cat: 12.4185791015625\n",
      "tabby, tabby cat: 9.224590301513672\n",
      "tiger cat: 8.243441581726074\n",
      "lynx, catamount: 6.76153564453125\n",
      "Siamese cat, Siamese: 5.189163684844971\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits # logits是模型的原始输出，表示每个类别的得分。\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item() # 使用argmax函数找到得分最高的类别索引。\n",
    "print(predicted_label)1\n",
    "print(model.config.id2label[predicted_label])\n",
    "\n",
    "# 打印前五个得分最高的类别以及它们的得分\n",
    "result = logits.topk(5, dim=-1)\n",
    "for i in range(5):\n",
    "    print(f\"{model.config.id2label[result.indices[0][i].item()]}: {result.values[0][i].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
